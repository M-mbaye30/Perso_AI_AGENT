"""
Agent principal de veille technologique NLP - Version corrig√©e
"""
import time
import logging
import traceback
from typing import List, Dict, Optional
from datetime import datetime

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nlp_agent.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Import de la configuration
from config import config, NLP_KEYWORDS, MAX_SEARCH_RESULTS

# Import des modules (noms corrig√©s)
from modules.search import search_nlp_papers, search_web
from modules.extraction import extract_content
from modules.analysis import analyze_nlp_relevance, extract_key_insights
from modules.synthesis import generate_tech_watch_report, save_reports
from modules.storage import save_search_results, save_analysis_results
from modules.security import validate_url


class TechWatchAgent:
    """Agent principal de veille technologique"""
    
    def __init__(self):
        self.config = config
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        logger.info(f"ü§ñ Agent initialis√© - Session: {self.session_id}")
    
    def run_full_cycle(self, custom_keywords: List[str] = None) -> Dict:
        """
        Lance un cycle complet de veille technologique
        
        Args:
            custom_keywords: Mots-cl√©s personnalis√©s (optionnel)
            
        Returns:
            Dict: Rapport de veille g√©n√©r√©
        """
        logger.info(f"üöÄ D√©marrage du cycle de veille - Session: {self.session_id}")
        
        cycle_data = {
            'session_id': self.session_id,
            'start_time': datetime.now(),
            'keywords_used': custom_keywords or NLP_KEYWORDS,
            'phases_completed': [],
            'errors': []
        }
        
        try:
            # Phase 1: Recherche
            search_results = self._search_phase(cycle_data)
            if not search_results:
                return self._create_error_report(cycle_data, "Aucun r√©sultat de recherche")
            
            # Phase 2: Extraction
            extracted_content = self._extraction_phase(search_results, cycle_data)
            if not extracted_content:
                return self._create_error_report(cycle_data, "Aucun contenu extrait")
            
            # Phase 3: Analyse
            analyzed_content = self._analysis_phase(extracted_content, cycle_data)
            if not analyzed_content:
                return self._create_error_report(cycle_data, "Aucune analyse r√©alis√©e")
            
            # Phase 4: Synth√®se et rapport
            report = self._synthesis_phase(analyzed_content, cycle_data)
            
            cycle_data['end_time'] = datetime.now()
            cycle_data['duration'] = (cycle_data['end_time'] - cycle_data['start_time']).total_seconds()
            
            logger.info(f"‚úÖ Cycle termin√© avec succ√®s en {cycle_data['duration']:.1f}s")
            return report
            
        except Exception as e:
            logger.error(f"‚ùå Erreur critique dans le cycle: {e}")
            logger.error(traceback.format_exc())
            return self._create_error_report(cycle_data, str(e))
    
    def _search_phase(self, cycle_data: Dict) -> List[Dict]:
        """Phase 1: Recherche web"""
        logger.info("üì° Phase 1: Recherche web")
        
        try:
            keywords = cycle_data['keywords_used']
            search_results = search_nlp_papers(keywords)
            
            if search_results:
                logger.info(f"Trouv√© {len(search_results)} r√©sultats de recherche")
                save_search_results(search_results, ", ".join(keywords))
                cycle_data['phases_completed'].append('search')
            else:
                logger.warning("Aucun r√©sultat de recherche trouv√©")
            
            return search_results
            
        except Exception as e:
            error_msg = f"Erreur phase recherche: {e}"
            logger.error(error_msg)
            cycle_data['errors'].append(error_msg)
            return []
    
    def _extraction_phase(self, search_results: List[Dict], cycle_data: Dict) -> List[Dict]:
        """Phase 2: Extraction de contenu"""
        logger.info("üìÑ Phase 2: Extraction de contenu")
        
        extracted_content = []
        successful_extractions = 0
        failed_extractions = 0
        
        try:
            for i, result in enumerate(search_results[:MAX_SEARCH_RESULTS]):
                try:
                    url = result.get('url')
                    logger.debug(f"Extraction {i+1}/{min(len(search_results), MAX_SEARCH_RESULTS)}: {url}")
                    
                    if not validate_url(url):
                        logger.warning(f"URL invalide ignor√©e: {url}")
                        failed_extractions += 1
                        continue
                    
                    content = extract_content(url)
                    if content:
                        # Enrichissement avec m√©tadonn√©es de recherche
                        content.update({
                            'search_title': result.get('title'),
                            'search_snippet': result.get('snippet'),
                            'search_timestamp': result.get('timestamp'),
                            'extraction_order': i + 1
                        })
                        extracted_content.append(content)
                        successful_extractions += 1
                    else:
                        failed_extractions += 1
                    
                    # Pause pour √©viter la surcharge
                    time.sleep(1)
                    
                except Exception as e:
                    logger.warning(f"√âchec extraction {url}: {e}")
                    failed_extractions += 1
                    continue
            
            logger.info(f"Extraction termin√©e: {successful_extractions} succ√®s, {failed_extractions} √©checs")
            
            if extracted_content:
                cycle_data['phases_completed'].append('extraction')
            
            return extracted_content
            
        except Exception as e:
            error_msg = f"Erreur phase extraction: {e}"
            logger.error(error_msg)
            cycle_data['errors'].append(error_msg)
            return []
    
    def _analysis_phase(self, extracted_content: List[Dict], cycle_data: Dict) -> List[Dict]:
        """Phase 3: Analyse NLP"""
        logger.info("üß† Phase 3: Analyse NLP")
        
        analyzed_content = []
        analysis_stats = {'total': 0, 'success': 0, 'failed': 0}
        
        try:
            for i, content in enumerate(extracted_content):
                try:
                    logger.debug(f"Analyse {i+1}/{len(extracted_content)}: {content.get('title', 'Sans titre')}")
                    
                    analysis = analyze_nlp_relevance(
                        content.get('content', ''),
                        content.get('title', '')
                    )
                    
                    analyzed_item = {
                        **content,
                        'analysis': analysis,
                        'analysis_timestamp': datetime.now().isoformat()
                    }
                    
                    analyzed_content.append(analyzed_item)
                    analysis_stats['success'] += 1
                    
                    logger.debug(f"Score de pertinence: {analysis.get('relevance_score', 0)}")
                    
                    # Pause pour respecter les limites API
                    time.sleep(2)
                    
                except Exception as e:
                    logger.warning(f"√âchec analyse document {i+1}: {e}")
                    analysis_stats['failed'] += 1
                    continue
                
                analysis_stats['total'] += 1
            
            logger.info(f"Analyse termin√©e: {analysis_stats['success']} succ√®s, {analysis_stats['failed']} √©checs")
            
            if analyzed_content:
                save_analysis_results(analyzed_content)
                cycle_data['phases_completed'].append('analysis')
                cycle_data['analysis_stats'] = analysis_stats
            
            return analyzed_content
            
        except Exception as e:
            error_msg = f"Erreur phase analyse: {e}"
            logger.error(error_msg)
            cycle_data['errors'].append(error_msg)
            return []
    
    def _synthesis_phase(self, analyzed_content: List[Dict], cycle_data: Dict) -> Dict:
        """Phase 4: Synth√®se et g√©n√©ration de rapport"""
        logger.info("üìä Phase 4: G√©n√©ration du rapport")
        
        try:
            # G√©n√©ration du rapport principal
            report = generate_tech_watch_report(analyzed_content)
            
            # Ajout des m√©tadonn√©es de session
            report['session_metadata'] = {
                'session_id': cycle_data['session_id'],
                'duration_seconds': (datetime.now() - cycle_data['start_time']).total_seconds(),
                'phases_completed': cycle_data['phases_completed'],
                'errors_count': len(cycle_data['errors'])
            }
            
            # Extraction des insights globaux
            try:
                logger.info("Extraction des insights globaux...")
                global_insights = extract_key_insights(analyzed_content)
                report['global_insights'] = global_insights
            except Exception as e:
                logger.warning(f"√âchec extraction insights globaux: {e}")
                report['global_insights'] = {}
            
            # Sauvegarde des rapports
            try:
                save_success = save_reports(report)
                if save_success:
                    logger.info("‚úÖ Rapports sauvegard√©s avec succ√®s")
                    cycle_data['phases_completed'].append('synthesis')
                else:
                    logger.warning("‚ö†Ô∏è Probl√®me lors de la sauvegarde des rapports")
            except Exception as e:
                logger.error(f"Erreur sauvegarde rapports: {e}")
            
            return report
            
        except Exception as e:
            error_msg = f"Erreur phase synth√®se: {e}"
            logger.error(error_msg)
            cycle_data['errors'].append(error_msg)
            return self._create_error_report(cycle_data, error_msg)
    
    def _create_error_report(self, cycle_data: Dict, error_message: str) -> Dict:
        """Cr√©e un rapport d'erreur"""
        return {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'report_id': f"error_{cycle_data['session_id']}",
                'status': 'error',
                'error_message': error_message
            },
            'session_metadata': cycle_data,
            'summary': {
                'total_documents_analyzed': 0,
                'relevant_documents': 0,
                'relevance_rate': 0.0,
                'average_relevance_score': 0.0
            }
        }
    
    def run_targeted_search(self, query: str, max_results: int = 10) -> Dict:
        """
        Recherche cibl√©e sur un sujet sp√©cifique
        
        Args:
            query: Requ√™te de recherche
            max_results: Nombre maximum de r√©sultats
            
        Returns:
            Dict: R√©sultats de la recherche cibl√©e
        """
        logger.info(f"üéØ Recherche cibl√©e: {query}")
        
        try:
            # Recherche
            results = search_web(query, max_results)
            
            if not results:
                return {'error': 'Aucun r√©sultat trouv√©', 'query': query}
            
            # Analyse rapide des premiers r√©sultats
            analyzed_results = []
            
            for result in results[:5]:  # Limiter √† 5 pour l'analyse rapide
                try:
                    url = result.get('url')
                    logger.debug(f"Analyse rapide: {url}")
                    
                    content = extract_content(url)
                    if content:
                        analysis = analyze_nlp_relevance(
                            content.get('content', ''),
                            content.get('title', '')
                        )
                        
                        analyzed_results.append({
                            **content,
                            'analysis': analysis,
                            'search_metadata': result
                        })
                    
                    time.sleep(1)  # Pause entre extractions
                    
                except Exception as e:
                    logger.warning(f"Erreur analyse rapide {result.get('url')}: {e}")
                    continue
            
            return {
                'query': query,
                'total_found': len(results),
                'analyzed_count': len(analyzed_results),
                'results': analyzed_results,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Erreur recherche cibl√©e: {e}")
            return {'error': str(e), 'query': query}


# Fonctions globales pour compatibilit√©
def run_tech_watch_cycle(custom_keywords: List[str] = None) -> Dict:
    """Lance un cycle complet de veille - Interface globale"""
    agent = TechWatchAgent()
    return agent.run_full_cycle(custom_keywords)

def run_targeted_search(query: str, max_results: int = 10) -> Dict:
    """Recherche cibl√©e - Interface globale"""
    agent = TechWatchAgent()
    return agent.run_targeted_search(query, max_results)

def monitor_keywords(keywords: List[str], check_interval: int = 3600) -> None:
    """
    Surveillance continue de mots-cl√©s
    
    Args:
        keywords: Liste des mots-cl√©s √† surveiller
        check_interval: Intervalle en secondes entre les v√©rifications
    """
    import schedule
    
    agent = TechWatchAgent()
    
    def check_keywords():
        logger.info(f"üîç V√©rification programm√©e - {datetime.now()}")
        try:
            report = agent.run_full_cycle(keywords)
            
            # Compter les documents tr√®s pertinents
            relevant_count = 0
            if 'detailed_analysis' in report:
                relevant_count = len([
                    item for item in report['detailed_analysis']
                    if item.get('analysis', {}).get('relevance_score', 0) > 8
                ])
            
            if relevant_count > 0:
                logger.info(f"üö® {relevant_count} documents tr√®s pertinents trouv√©s!")
            else:
                logger.info("‚ÑπÔ∏è Aucun document tr√®s pertinent trouv√©")
                
        except Exception as e:
            logger.error(f"Erreur surveillance programm√©e: {e}")
    
    try:
        # Programmation de la surveillance
        schedule.every(check_interval).seconds.do(check_keywords)
        logger.info(f"üìÖ Surveillance programm√©e toutes les {check_interval/3600:.1f}h")
        
        # Boucle principale
        while True:
            schedule.run_pending()
            time.sleep(60)  # V√©rifier chaque minute
            
    except KeyboardInterrupt:
        logger.info("üõë Surveillance interrompue par l'utilisateur")
    except Exception as e:
        logger.error(f"Erreur boucle surveillance: {e}")


if __name__ == "__main__":
    try:
        # Test de l'agent
        agent = TechWatchAgent()
        report = agent.run_full_cycle()
        
        if 'error_message' not in report.get('metadata', {}):
            print(f"‚úÖ Rapport g√©n√©r√©: {report.get('metadata', {}).get('report_id', 'unknown')}")
        else:
            print(f"‚ùå Erreur: {report['metadata']['error_message']}")
            
    except Exception as e:
        logger.error(f"Erreur ex√©cution principale: {e}")
        print(f"‚ùå Erreur critique: {e}")